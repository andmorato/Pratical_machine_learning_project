---
title: "Machine Learning - Final Project"
author: "Andre Morato"
date: "10 de abril de 2017"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

## Introduction

In this report it will be applied some of techniques teached in class of Machine Learning. The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

### Data

It was provided a training data for model development. The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

Let's load the data and provide some basic information.

```{r, fig.width=9, warning=FALSE, message=FALSE}
## Loading used packages
library(caret)
data<-read.csv("pml-training.csv", header = TRUE, na.strings = c("","NA"))
dim(data)
head(names(data))
tail(names(data))
```

It will be developed a model for predict the "classe" of activity. Let's see how each label of "classe" is distributed in data set.

```{r}
class(data$classe)
table(data$classe)
```

There are five possible outcomes for the activity. The label "A" is the most frequent and the others ("B", "C", "D" and "E") have similar frequencies.

It will be a subset in data to remove variables which all measurements is equal to NA.

```{r}
## Finding the variables that contains NA
na <- NULL
for (i in 1:159){
  na<-c(na,sum(is.na(data[,i])))
}
cleandata <- data[,na==0]
dim(cleandata)
sum(is.na(cleandata))
```

The first two variables will be removed, so the only predictors will be the acelerometers measurement.

```{r}
cleandata<-cleandata[,-c(1,2)]
```


### Data partition

There are **`r dim(cleandata)[2]-1`** variables and **`r dim(cleandata)[1]`** observations. It is a large sample size, so it is possible to apply testing and validation of proposed models. The following distribution will be applied.

- 60% training
- 20% testing
- 20% validation

The following code creates the training, testing and validation data sets.

```{r, fig.width=9, fig.height = 3, fig.align='center'}
inTrain <- createDataPartition(y=cleandata$classe, p=0.6, list=FALSE)
training <- cleandata[inTrain,]
buildData <- cleandata[-inTrain,]

inTest <- createDataPartition(y=buildData$classe, p = 0.5, list=FALSE)
testing <- buildData[inTest,]
validation <- buildData[-inTest,]

dim(training)
dim(testing)
dim(validation)
```

### Model description

The chosen method is combining predictors. This approach tends to increase the accuracy. One major flop of this method is the reduction in interpretability.

The fisrt model chosen is a random forest. This model can be seen as a variation of combining predictors. The pros and cons are the same pointed for combining predictors approach.

The random forest method is applied with cross validation K-fold type. The number of folds is 10, that is the standard when choosing cross validation in caret package. The code below create the first model.

```{r, message=FALSE}
mod1 <- train(classe ~ ., method = "rf", data = training, trControl = trainControl(method = "cv"))
mod1
```

This model achieved a very high accurace in training set. This could be a sign of overfitting. A perfect in sample predictor predict both signal and noise of data. So, it is expected a not so good performance in new sample.

Let's evaluate this model in test data.

```{r, message=FALSE}
pred1 <- predict(mod1, testing)
confusionMatrix(pred1, testing$classe)
```

Surprisingly, it can be seen an almost perfect prediction in test set. The expectation of a bad performance in new data was not confirmed. This good result in test set allow us to don't make changes in model before check it in validation data.

Now, let's check the model with in validation data.

```{r, message=FALSE}
valid1 <- predict(mod1, validation)
confusionMatrix(valid1, validation$classe)
```

Again, it can be seen an almost perfect prediction in validation set too. The expectation of a bad performance in new data was not confirmed again. This good result in validation set allow us to abort the planned strategy and adopt this random forest model for predict the test set provided.

### Prediction of 20 test cases

Now, we will predict the "classe" of 20 test cases.

```{r, message=FALSE}
## Loading test set
testset<-read.csv("pml-testing.csv", header = TRUE, na.strings = c("","NA"))
dim(testset)

cleantest <- testset[,na==0]
dim(cleantest)
sum(is.na(cleantest))
cleantest<-cleantest[,-c(1,2)]

## Prediction with the model
finalpred <- predict(mod1, cleantest)
finalpred
```

**All predictions are correct! It was verified in the quiz.**


The following code will generate a illustration of the top of decision tree corresponding to the model created. This code was obtaind in http://stats.stackexchange.com/questions/2344/best-way-to-present-a-random-forest-in-a-publication

```{r, fig.width=20, fig.height = 10, fig.align='center'}
to.dendrogram <- function(dfrep,rownum=1,height.increment=0.1){

  if(dfrep[rownum,'status'] == -1){
    rval <- list()

    attr(rval,"members") <- 1
    attr(rval,"height") <- 0.0
    attr(rval,"label") <- dfrep[rownum,'prediction']
    attr(rval,"leaf") <- TRUE

  }else{##note the change "to.dendrogram" and not "to.dendogram"
    left <- to.dendrogram(dfrep,dfrep[rownum,'left daughter'],height.increment)
    right <- to.dendrogram(dfrep,dfrep[rownum,'right daughter'],height.increment)
    rval <- list(left,right)

    attr(rval,"members") <- attr(left,"members") + attr(right,"members")
    attr(rval,"height") <- max(attr(left,"height"),attr(right,"height")) + height.increment
    attr(rval,"leaf") <- FALSE
    attr(rval,"edgetext") <- dfrep[rownum,'split var']
    #To add Split Point in Dendrogram
    #attr(rval,"edgetext") <- paste(dfrep[rownum,'split var'],"\n<",round(dfrep[rownum,'split point'], digits = 2),"=>", sep = " ")
  }

  class(rval) <- "dendrogram"

  return(rval)
}

tree <- getTree(mod1$finalModel,1,labelVar=TRUE)

d <- to.dendrogram(tree)
## str(d)
plot(d,center=TRUE,leaflab='none',edgePar=list(t.cex=1,p.col=NA,p.lty=0))
```
